[
{
	"uri": "https://databrickslabs.github.io/delta-oms/faq/general/",
	"title": "General",
	"tags": [],
	"description": "",
	"content": "General Q. What is Delta Operational Metrics Store ?\nDelta Operational metrics store (DeltaOMS) is a solution/framework for automated collection and tracking of Delta commit logs and other future operational metrics from Delta Lake, build a centralized repository for Delta Lake operational statistics and simplify analysis across the entire data lake.\nThe solution can be easily enabled and configured to start capturing the operational metrics into a centralized repository on the data lake. Once the data is collated , it would unlock the possibilities for gaining operational insights, creating dashboards for traceability of operations across the data lake through a single pane of glass and other analytical use cases.\nQ. What are the benefits of using DeltaOMS ?\nTracking and analyzing Delta Lake operational metrics across multiple database objects requires building a custom solution on the Delta Lakehouse.DeltaOMS helps to automate the collection of operational logs from multiple Delta Lake objects, collate those into a central repository on the lakehouse , allow for more holistic analysis and allow presenting them through a single pane of glass dashboard for typical operational analytics. This simplifies the process for users looking to gain insights into their Delta Lakehouse table operations.\nQ. What typical operational insights would I get from the solution ?\nDeltaOMS centralized repository provides interfaces for custom analysis on the Delta Lake operational metrics using tools like Apache Spark, Databricks SQL Analytics etc.\nFor example, it could answer questions like :\n What are the most frequent WRITE operations across my data lake ? Which are the expensive WRITE operations on my lake ? How many WRITE operations were run on my Data Lake in the last hour ? What is the average duration of WRITE operations on my database Delta tables ? Which of the WRITE operations involve inserting the largest amount of data ? Which are the top WRITE heavy databases in my data lake ? Track File I/O ( bytes written, number of writes etc.) across my entire data lake Tracking growth of data size, commit frequency etc. over time for tables/databases Track changes over time for Delta operations/data Did the delete operations for GDPR compliance go through and what changes it made to the filesystem ? And many more \u0026hellip;  Q. Who should use this feature ?\nData Engineering teams, Data Lake Admins and Operational Analysts would be able to manage and use this feature for operational insights on the Delta Lake.\nQ. How will I be charged ?\nThis solution is fully deployed in the users environment. The automated jobs for the framework will run on the users Databricks environment.Depending on the configuration set by the users (for example, update frequency of the audit logs, number of databases/delta path enabled etc.), the cost of the automated jobs and associated storage cost will vary.\n"
},
{
	"uri": "https://databrickslabs.github.io/delta-oms/getting_started/",
	"title": "Getting Started",
	"tags": [],
	"description": "",
	"content": "DeltaOMS - Getting Started The following tutorial will guide you through the process for setting up the Delta Operational Metrics Store (DeltaOMS) on your Databricks Lakehouse environment.\nYou will deploy/configure the solution, configure a database/table for metrics collection, execute DeltaOMS to collate the metrics and finally run some sample analysis on the processed data.\n"
},
{
	"uri": "https://databrickslabs.github.io/delta-oms/developer_guide/configurationtables/",
	"title": "Configuration Tables",
	"tags": [],
	"description": "",
	"content": "Configuration Tables Source Config Default Name : sourceconfig\nTable used for adding databases/paths/individual table to be tracked by DeltaOMS\n   Column Type Description     path String Path to the Delta object to be tracked by DeltaOMS. Could be a database (all tables will be included), directory, individual table or specific path   skipProcessing Boolean Flag to exclude processing a row   parameters Map\u0026lt;String,String\u0026gt; Placeholder for dynamic parameters to be passed to DeltaOMS (supports easy future expansion). Currently , the only required parameter is wildCardLevel    Typical usage for adding input source configuration :\n-- Adding a Database. OMS will discover all Delta tables in the database. -- ConfigurePaths process could take longer depending on number of tables in the database. INSERT INTO \u0026lt;OMSDBNAME\u0026gt;.sourceconfig values('\u0026lt;databaseName\u0026gt;', false, Map('wildCardLevel','0')) -- Adding an entire directory. OMS will discover all Delta tables underneath the directory recursively. -- The `**` is required after the directory name. -- ConfigurePaths process could take longer depending on number of nested delta paths under the directory. INSERT INTO \u0026lt;OMSDBNAME\u0026gt;.sourceconfig values('\u0026lt;dirName/**\u0026gt;', false, Map('wildCardLevel','0')) -- Adding a table INSERT INTO \u0026lt;OMSDBNAME\u0026gt;.sourceconfig values('\u0026lt;fully qualified table name\u0026gt;', false, Map('wildCardLevel','0')) -- Adding an individual table path INSERT INTO \u0026lt;OMSDBNAME\u0026gt;.sourceconfig values('\u0026lt;Full table path on storage\u0026gt;', false, Map('wildCardLevel','0')) WildCard Level Configuration The wildcard level setting for a path determines how the path is modified into a wildcard path internally and used by DeltaOMS during ingestion.\n   Param Value Description Example     Map('wildCardLevel','0') Tracks all tables in a database path dbfs:/user/hive/warehouse/sample.db/table1 -\u0026gt; dbfs:/user/hive/warehouse/sample.db/*/_delta_log/*.json   Map('wildCardLevel','1') Tracks multiple database under a path dbfs:/user/hive/warehouse/sample.db/table1 -\u0026gt; dbfs:/user/hive/warehouse/*/*/_delta_log/*.json   Map('wildCardLevel','-1') Tracks only the individual path dbfs:/user/hive/warehouse/sample.db/table1 -\u0026gt; dbfs:/user/hive/warehouse/sample.db/table1/_delta_log/*.json    Path Config Default Name : pathconfig\nInternal table that maintains the configuration for each individual Delta table path. This is populated by executing the com.databricks.labs.deltaoms.init.ConfigurePaths.main process\n   Column Type Description     path String Path to a individual Delta table to be tracked by DeltaOMS   puid String Unique identifier for the table path . Used for partitioning of data inside DeltaOMS   wildCardPath String Wildcard path representation from the individual table path . Used for generating optimized read streams. Behaviour controlled through the wildCardLevel parameter   wuid String Unique identifier for wildcard path. Used for uniquely identifying the streams   qualifiedName String Fully Qualified name of a table (if defined in the metastore). For tables not defined in metastore, defaults to the table path   parameters Map\u0026lt;String,String\u0026gt; Placeholder for dynamic parameters to be passed to DeltaOMS (supports easy future expansion). Currently , the only required parameter is wildCardLevel    Processed History Default Name : processedhistory\nInternal tracking table for last version of Delta actions processed by DeltaOMS. Populated during OMS data processing\n   Column Type Description     tableName String Table name tracked by DeltaOMS. Eg. - rawactions   lastVersion Long Last Version of Actions processed by DeltaOMS   update_ts Timestamp Last update timestamp    "
},
{
	"uri": "https://databrickslabs.github.io/delta-oms/developer_guide/",
	"title": "Developer Guide",
	"tags": [],
	"description": "",
	"content": "DeltaOMS - Developer Guide DeltaOMS provides \u0026ldquo;Automated Observability\u0026rdquo; for Delta Lakehouse objects. This developer guide assumes you have already completed the tutorial from the Getting Started Guide.\nOnce you have completed the getting started , this guide will help with understanding inner workings of DeltaOMS\n"
},
{
	"uri": "https://databrickslabs.github.io/delta-oms/developer_guide/datatables/",
	"title": "Data Tables",
	"tags": [],
	"description": "",
	"content": "Data Tables Raw Actions Default Name : rawactions\nStores the raw actions captured through the ingestion of the _delta_log json files for all tracked tables. The schema has the columns matching the Actions from here(https://github.com/delta-io/delta/blob/master/core/src/main/scala/org/apache/spark/sql/delta/actions/actions.scala#L515) and the Delta Log Protocol with the following additional fields :\n   Column Type Description     file_name String Name of the Delta log transaction json file. Eg. - dbfs:/user/hive/warehouse/sample.db/table1/_delta_log/00000000000000000025.json   path String Path to the Delta table. Eg. - dbfs:/user/hive/warehouse/sample.db/table1   puid String Path Unique Identifier (Partition column)   commit_version Long Transaction Commit Version of the data. Eg. - 25   commit_ts Timestamp Transaction Commit Timestamp. Eg. - 2021-06-16T18:08:20.000+0000   update_ts Timestamp Last update timestamp   commit_date Date Transaction Commit Date (Partition column)    Commit Info Snapshots Default Name : commitinfosnapshots\nDeltaOMS processes the raw actions from the Delta logs and creates a separate table for the Commit Information The schema matches the history schema with the following additional columns:\n   Column Type Description     file_name String Name of the Delta log transaction json file. Eg. - dbfs:/user/hive/warehouse/sample.db/table1/_delta_log/00000000000000000025.json   path String Path to the Delta table. Eg. - dbfs:/user/hive/warehouse/sample.db/table1   puid String Path Unique Identifier (Partition column)   commit_version Long Transaction Commit Version of the data. Eg. - 25   commit_ts Timestamp Transaction Commit Timestamp. Eg. - 2021-06-16T18:08:20.000+0000   update_ts Timestamp Last update timestamp   commit_date Date Transaction Commit Date (Partition column)    Action Snapshots Default Name : actionsnapshots\nDeltaOMS ingests all the actions from the tracked delta transaction logs. During processing, DeltaOMS extracts the Add/Remove actions, reconciles the AddFile and RemoveFile actions to build the snapshots at each commit_version for a table/path and populates the action snapshots table.\nThis table provides the ability to query file snapshots for any tracked delta path at certain point in time / commit version and get the data file details.\n   Column Type Description     data_path String Path to the Delta table. Eg. - dbfs:/user/hive/warehouse/sample.db/table1   puid String Path Unique Identifier (Partition column)   commit_version Long Transaction Commit Version of the data. Eg. - 25   commit_ts Timestamp Transaction Commit Timestamp. Eg. - 2021-06-16T18:08:20.000+0000   commit_date Date Transaction Commit Date (Partition column)   add_file Struct AddFile    "
},
{
	"uri": "https://databrickslabs.github.io/delta-oms/faq/execution/",
	"title": "Execution",
	"tags": [],
	"description": "",
	"content": "Execution Q. How do I get started ?\nPlease refer to the Getting Started guide\nQ. How do I add databases to be monitored by DeltaOMS ?\nYou can add a database name to the DeltaOMS configuration table (by default called sourceconfig) using simple SQL INSERT statement.\nExample:\nINSERT INTO \u0026lt;omsDBName\u0026gt;.sourceconfig VALUES('\u0026lt;Database Name\u0026gt;',false, Map('wildCardLevel','0'))\nFor more details on the configurations and parameters, refer to Getting Started and Developer Guide\nQ. What components will to be deployed for DeltaOMS ?\nDeltaOMS deploys two primary components. One for ingestion of the delta logs from the configured table. This is a streaming component and can be run either on a schedule or as an always running streaming job depending on your SLA requirements for retrieving operational metrics.\nThe other component is a batch component for processing and enriching of the delta actions into different OMS specific tables.\nQ. How many ingestion jobs we need to run ?\nDeltaOMS supports 50 streams by default for each Databricks jobs. These values are configurable through command line parameters --startingStream and --endingStream, default 1 and 50 respectively.\nWe recommend setting up your jobs to support groups of 40-50 stream / wildcard paths. For example, you have 75 unique wildcard paths to process, we recommend creating 2 Databricks jobs for DeltaOMS ingestion. The sample notebook provides examples of how to automatically create jobs based on your input sources.\nQ. What is the process flow for adding new input sources for DeltaOMS tracking ? Assuming you already have DeltaOMS running on your environment,new input sources can be added by:\n Adding the new sources to the sourceconfig table. Example usage in Developer Guide Running the path configuration component, com.databricks.labs.deltaoms.init.ConfigurePaths If running as an always running streaming job, restart the DeltaOMS streaming job(s) If running as a scheduled job, new sources will be automatically picked up during subsequent runs  Q. (Advanced) How do I add arbitrary wildcard paths to be tracked by DeltaOMS ?\nWe recommend using the sourceconfig configuration table to set up input source tracking for DeltaOMS (Refer to Developer Guide\nThere could be instances where some special wildcard path (which does not fall under the wildCardLevel provided by DeltaOMS) needs to be tracked by DeltaOMS. These random wildcards can be configured directly on the pathconfig configuration table.\nExample: Say, you need to configure paths like dbfs:/databricks-datasets/*/*/*/*/*/_delta_log/*.json,dbfs:/databricks-datasets/*/*/*/*/_delta_log/*.json and dbfs:/databricks-datasets/*/*/_delta_log/*.json. You could directly add them to the pathconfig table using the SQL statements\nspark.sql(s\u0026quot;\u0026quot;\u0026quot;INSERT INTO \u0026lt;omsDBName\u0026gt;.pathconfig VALUES ( 'dbfs:/databricks-datasets/4/', substring(sha1('dbfs:/databricks-datasets/4/'), 0, 7), 'dbfs:/databricks-datasets/*/*/*/*/*/_delta_log/*.json', substring(sha1('dbfs:/databricks-datasets/*/*/*/*/*/_delta_log/*.json'), 0, 7), Map('wildCardLevel','1'),false,'databricks-datasets-4',0,false,'2021-07-23T19:06:04.933+0000') \u0026quot;\u0026quot;\u0026quot;) Once these are added , the operational metrics from tables under these wildcard location can be captured and processed using the regular OMS jobs.\n"
},
{
	"uri": "https://databrickslabs.github.io/delta-oms/getting_started/prerequisites/",
	"title": "Pre-Requisites",
	"tags": [],
	"description": "",
	"content": "Prerequisites Make sure you have the following available before proceeding :\n Access to Databricks environment with access to Delta tables/databases Ability to create Databricks job and run them on new cluster Proper access permissions to create database, tables and write data to the desired OMS location Procured the Delta OMS solution package from your Databricks team (includes DeltaOMS jar , sample notebooks and documentations) Databricks Runtime 8.3+  "
},
{
	"uri": "https://databrickslabs.github.io/delta-oms/faq/",
	"title": "FAQ",
	"tags": [],
	"description": "",
	"content": "Frequently Asked Questions "
},
{
	"uri": "https://databrickslabs.github.io/delta-oms/developer_guide/datamodels/",
	"title": "Data Model",
	"tags": [],
	"description": "",
	"content": "Data Model "
},
{
	"uri": "https://databrickslabs.github.io/delta-oms/getting_started/setup/",
	"title": "Setup",
	"tags": [],
	"description": "",
	"content": "Setup Initialize the DeltaOMS Database DeltaOMS can be configured through two methods :\n Command line parameters - Limited to few basic mandatory configurations Configuration file - Access to all the configuration options. Refer to Additional Configurations section below for full details  For this tutorial we will use the first option.Follow the below steps to initialize the DeltaOMS centralized Database and tables.\n  Import and Open the DeltaOMS Setup notebook STEP 1 into your Databricks environment\n  Modify the value of the variables omsBaseLocation, omsDBName, omsCheckpointSuffix, omsCheckpointBase as appropriate for your environment\n   Variable Description     omsBaseLocation Base location/path of the OMS Database on the Delta Lakehouse   omsDBName DeltaOMS Database Name. This is the centralized database with all the Delta logs   omsCheckpointBase DeltaOMS ingestion is a streaming process.This defines the Base path for the checkpoints   omsCheckpointSuffix Suffix to be added to the checkpoint path (Helps in making the path unique)      Execute com.databricks.labs.deltaoms.init.InitializeOMS.main method to create the OMS DB and tables.\n  Validate the DeltaOMS database and tables were created\n  Configure Delta Lakehouse objects for DeltaOMS tracking Next, we will add few input sources (existing Delta databases or tables) to be tracked by DeltaOMS. This is done using the same notebook.\n  Add the names of few databases you want to track via DeltaOMS to the sourceconfig table in the DeltaOMD DB. This is done by using a simple SQL INSERT statement:\nINSERT INTO \u0026lt;omsDBName\u0026gt;.sourceconfig VALUES('\u0026lt;Database Name\u0026gt;',false, Map('wildCardLevel','0'))\nRefer to the Developer Guide for more details on the tables.\n  Configure the internal DeltaOMS configuration tables by executing com.databricks.labs.deltaoms.init.ConfigurePaths.main. This will populate the internal configuration table pathconfig with the detailed path information for all delta tables under the database\n  Create Databricks Jobs Next, we will create couple of databricks jobs to stream the delta logs from the tracked tables and also to process the data for further analytics.\n Import and Open the DeltaOMS Setup notebook STEP 2 into your Databricks environment Define the values for the omsBaseLocation, omsDBName, omsCheckpointSuffix, omsCheckpointBase Upload the DeltaOMS jar to a cloud path of your choice and copy the full path Modify the provided Job Creation Json template and variables as appropriate to your environment. Make sure, the correct path of the jar is reflected in the oms_jar_location variable DeltaOMS creates individual streams for each tracked path and runs multiple such streams in a single Databricks job. By default, it groups 50 streams into a single databricks jobs. You could change the variable num_streams_per_job to change number of streams per job. Once all the parameters are updated, run the command on the notebok to create the jobs. Depending on the total number of objects tracked multiple Databricks jobs could be created You can navigate to the Jobs UI to look at the created jobs  Refer to the Developer Guide for more details on multiple stream approach for DeltaOMS ingestion.\n"
},
{
	"uri": "https://databrickslabs.github.io/delta-oms/contributing/",
	"title": "Contributing",
	"tags": [],
	"description": "",
	"content": "CONTRIBUTING TO DELTAOMS\nAt present, external contributions are not accepted\nInternal Databricks employees are welcome to contribute but are currently limited to tests and supporting refactorings until sufficient tests are in place. If you\u0026rsquo;re interested in developing tests please do reach out or look for issues labeled, \u0026ldquo;TestsNeeded\u0026rdquo;\n"
},
{
	"uri": "https://databrickslabs.github.io/delta-oms/developer_guide/components/",
	"title": "Key Components",
	"tags": [],
	"description": "",
	"content": "Key Components Initialization DeltaOMS provides the component com.databricks.labs.deltaoms.init.InitializeOMS for initializing\nthe centralized OMS database. The component creates the OMS DB at the location specified by the configuration settings. Note: This process will delete all existing data in the specified location.\n   Configuration Key Command Line Argument Description Required Example Default Value     base-location \u0026ndash;baseLocation= Base location/path of the OMS Database on the Delta Lake Y dbfs:/spark-warehouse/oms.db None   db-name \u0026ndash;dbName= OMS Database Name. This is the database where all the Delta log details will be collected Y oms.db None   raw-action-table None OMS table name for storing the raw delta logs collected from the configured tables N oms_raw_actions rawactions   source-config-table None Configuration table name for setting the list of Delta Path, databases and/or tables for which the delta logs should be collected by OMS N oms_source_config sourceconfig   path-config-table None Configuration table name for storing Delta path details and few related metadata for internal processing purposes by OMS N oms_path_config pathconfig   processed-history-table None Configuration table name for storing processing details for OMS ETL Pipelines. Used internally by OMS N oms_processed_history processedhistory   commit-info-snapshot-table None Table name for storing the Delta Commit Information generated from the processed raw Delta logs for configured tables/paths N oms_commitinfo_snapshots commitinfosnapshots   action-snapshot-table None Table name for storing the Delta Actions information snapshots. Generated from processing the Raw Delta logs N oms_action_snapshots actionsnapshots    Configuration Once DeltaOMS has been initialized, and the input sources configured Source Config, the component provided by com.databricks.labs.deltaoms.init.ConfigurePaths is executed to populate the internal path config tables. This component is also responsible for discovering delta tables under a database or under a directory (based on how the source is configured).\n   Configuration Key Command Line Argument Description Required Example Default Value     base-location \u0026ndash;baseLocation= Base location/path of the OMS Database on the Delta Lake Y dbfs:/spark-warehouse/oms.db None   db-name \u0026ndash;dbName= OMS Database Name. This is the database where all the Delta log details will be collected Y oms.db None   skip-initialize-oms \u0026ndash;skipInitializeOMS Skip initializing DeltaOMS DB N true false    This component should be executed whenever there are updates to the source config table.\nIngestion DeltaOMS uses the delta log json files as its input. Details of the Delta Protocol Json files can be found here\nBased on the databases, tables or individual path configured in DeltaOMS, it will fetch the json files under the _delta_log folder as streams. To prevent creation of multiple streams (applicable for large number of databases and streams) , the number of streams are optimized by creating streams on wildcard paths instead of individual paths for table(s).\nFor example, if a tracked table has the path as dbfs:/user/hive/warehouse/sample.db/table1, DeltaOMS uses the path dbfs:/user/hive/warehouse/sample.db/*/_delta_log/*.json to read the Delta logs.\nThe benefits of this approach are :\n Automatically supports any additional table added to the database No additional streams needs to be created for each additional table, making the solution more scalable  For ways to configure the wildcard behaviour , refer to Source Config\nDuring ingestion, DeltaOMS reads the list of wildcardpaths from the pathconfig table and creates separate read streams for each path. OMS also maintains separate checkpoint location, pool and queryname for each stream. The Delta log json files are read, enriched with additional fields and persisted to the rawactions table.\nThe ingestion process is executed from the com.databricks.labs.deltaoms.ingest.StreamPopulateOMS object. By default, each streaming ingestion job can support upto 50 streams. If more than 50 streams ( for example more than 50 distinct wildcard paths) would be involved , we recommend creating separate Databricks jobs. The com.databricks.labs.deltaoms.ingest.StreamPopulateOMS supports command line parameters --startingStream and --endingStream for setting the number of streams in each job.By default, these are set to 1 and 50 respectively.\n   Configuration Key Command Line Argument Description Required Example Default Value     base-location \u0026ndash;baseLocation= Base location/path of the OMS Database on the Delta Lake Y dbfs:/spark-warehouse/oms.db None   db-name \u0026ndash;dbName= OMS Database Name. This is the database where all the Delta log details will be collected Y oms.db None   checkpoint-base \u0026ndash;checkpointBase= Base path for the checkpoints for OMS streaming pipeline for collecting the Delta logs for the configured tables Y dbfs:/_oms_checkpoints/ None   checkpoint-suffix \u0026ndash;checkpointSuffix= Suffix to be added to the checkpoint path. Useful during testing for starting off a fresh process Y _1234 None   trigger-interval None Trigger interval for processing the Delta logs from the configured tables/paths N 30s Once   starting-stream \u0026ndash;startingStream= Starting stream number for the Ingestion Job N 10 1   ending-stream \u0026ndash;endingStream= Ending stream number for the Ingestion Job N 30 50   skip-path-config \u0026ndash;skipPathConfig Skip updating the Path Config table from the latest Source config N true false   skip-initialize-oms \u0026ndash;skipInitializeOMS Skip initializing DeltaOMS DB N true false    Processing DeltaOMS processes the ingested data from rawactions and creates enriched / reconciled tables for Commit Info and Snapshots.\nThe com.databricks.labs.deltaoms.process.OMSProcessRawActions object looks for the newly added raw actions by utilizing the Change Data Feed feature. The new actions are reconciled to get a list of data files conforming to AddFile and CommitInfo. These are then persisted into Action Snapshots and Commit Info Snapshots respectively.\n   Configuration Key Command Line Argument Description Required Example Default Value     base-location \u0026ndash;baseLocation= Base location/path of the OMS Database on the Delta Lake Y dbfs:/spark-warehouse/oms.db None   db-name \u0026ndash;dbName= OMS Database Name. This is the database where all the Delta log details will be collected Y oms.db None    "
},
{
	"uri": "https://databrickslabs.github.io/delta-oms/getting_started/execute/",
	"title": "Execute",
	"tags": [],
	"description": "",
	"content": "Execute Execute the DeltaOMS Jobs You can run the jobs created in the above step to ingest and process the delta transaction information for the configured tables into the centralized DeltaOMS database.\nBy default, the OMS_Streaming_Ingestion_* jobs bring in the raw delta logs from the configured databases/tables. The OMS_ProcessRawActions* job formats and enrich the raw delta log data.\nFor more details refer to the Developer Guide\n"
},
{
	"uri": "https://databrickslabs.github.io/delta-oms/getting_started/analyze/",
	"title": "Analyze",
	"tags": [],
	"description": "",
	"content": "Analyze Run analytics using sample notebooks You can run some sample analytics on the data from the OMS database using the provided notebook\nModify the queries to reflect your configuration for OMS DB Name and other table names.\nExecuting this notebook will give you an idea on the type of analysis and data structure that can be utilized as part of the DeltaOMS.\nYou can build on top of this notebook , customize this notebook to your liking and create your own Analytics insights and dashboards through Databricks notebooks and/or SQL Analytics.\n"
},
{
	"uri": "https://databrickslabs.github.io/delta-oms/getting_started/additionalconfigurations/",
	"title": "Additional Configuration",
	"tags": [],
	"description": "",
	"content": "Additional Configurations and Execution Options Configuration DeltaOMS can also be configured using a HOCON format file. The different configuration parameters available are described below :\n   Configuration Key Description Required Example Default Value     base-location Base location/path of the OMS Database on the Delta Lake Y dbfs:/spark-warehouse/oms.db None   db-name OMS Database Name. This is the database where all the Delta log details will be collected Y oms.db None   checkpoint-base Base path for the checkpoints for OMS streaming pipeline for collecting the Delta logs for the configured tables Y dbfs:/_oms_checkpoints/ None   checkpoint-suffix Suffix to be added to the checkpoint path. Useful during testing for starting off a fresh process Y _1234 None   raw-action-table OMS table name for storing the raw delta logs collected from the configured tables N oms_raw_actions rawactions   source-config-table Configuration table name for setting the list of Delta Path, databases and/or tables for which the delta logs should be collected by OMS N oms_source_config sourceconfig   path-config-table Configuration table name for storing Delta path details and few related metadata for internal processing purposes by OMS N oms_path_config pathconfig   processed-history-table Configuration table name for storing processing details for OMS ETL Pipelines. Used internally by OMS N oms_processed_history processedhistory   commit-info-snapshot-table Table name for storing the Delta Commit Information generated from the processed raw Delta logs for configured tables/paths N oms_commitinfo_snapshots commitinfosnapshots   action-snapshot-table Table name for storing the Delta Actions information snapshots. Generated from processing the Raw Delta logs N oms_action_snapshots actionsnapshots   consolidate-wildcard-path Flag to enable/disable processing Delta logs using consolidated wildcard patterns extracted from the path configured for OMS N false true   trigger-interval Trigger interval for processing the Delta logs from the configured tables/paths N 30s Once   src-database Comma separated list of Source database used for filtering when extracting the Delta table path information from metastore N Sample_db,test_db    table-pattern Wildcard filtering of tables to be extracted from the metastore for configuring the Delta OMS solution N *oms* *   starting-stream Starting stream number for the Ingestion Job N 10 1   ending-stream Ending stream number for the Ingestion Job N 30 50    A sample configuration file :\nbase-location=\u0026quot;dbfs:/home/warehouse/oms/\u0026quot; db-name=\u0026quot;oms_sample\u0026quot; raw-action-table=\u0026quot;raw_actions\u0026quot; source-config-table =\u0026quot;source_config\u0026quot; path-config-table=\u0026quot;path_config\u0026quot; processed-history-table=\u0026quot;processed_history\u0026quot; commit-info-snapshot-table=\u0026quot;commitinfo_snapshots\u0026quot; action-snapshot-table=\u0026quot;action_snapshots\u0026quot; consolidate-wildcard-path = \u0026quot;true\u0026quot; checkpoint-base=\u0026quot;dbfs:/home/oms/_checkpoints\u0026quot; checkpoint-suffix=\u0026quot;_sample_123900\u0026quot; trigger-interval=\u0026quot;once\u0026quot; You could use the above information to create a configuration file appropriate to your environment. The configuration file can be uploaded to DBFS or your cloud storage provider. Tip: You could directly create the configuration file on DBFS from a notebook using dbutils APIs\nExecution The setup and execution of DeltaOMS can also be configured through the above configuration file instead of command line parameters. The configuration approach provides you access to more parameters within DeltaOMS.\nOnce the config file has been created and uploaded to a cloud storage, either the earlier provided notebooks or jobs can be utilized for running the setup and execution.\nOn a notebook, the configuration can be provided by setting the system property OMS_CONFIG_FILE. For example, the initialization of DeltaOMS can be executed as follows :\nSystem.setProperty(\u0026quot;OMS_CONFIG_FILE\u0026quot;, \u0026quot;\u0026lt;\u0026lt;Full path to the configuration file\u0026gt;\u0026gt;\u0026quot;) val args = Array.empty[String] com.databricks.labs.deltaoms.init.InitializeOMS.main(args) For a job, the DeltaOMS configuration file can be added to the system path by modifying the cluster for the job and setting the advanced Spark configuration\nspark.driver.extraJavaOptions -DOMS_CONFIG_FILE=\u0026lt;\u0026lt;FULL PATH to the OMS Configuration file\u0026gt;\u0026gt;\nRefer to screenshots below for an example:\n"
},
{
	"uri": "https://databrickslabs.github.io/delta-oms/",
	"title": "Delta Operational Metrics Store (DeltaOMS)",
	"tags": [],
	"description": "",
	"content": "Delta Operational Metrics Store (DeltaOMS) DeltaOMS is a solution providing \u0026ldquo;Automated Observability\u0026rdquo; on Delta Lake\nProject Overview DeltaOMS provides a solution for automatically collecting operational metrics from Delta Lakehouse tables into a separate centralized database. This will enable you to gain access to the operational metadata of your data in near real-time and help with operational insights, allow setting up monitoring and alerting for your Delta Lakehouse ETL Pipelines, identify trends based on data characteristics, improve ETL pipeline performance etc. It also provides you the capabilities for auditing and traceability of your Delta Lake data.\nHow does DeltaOMS work DeltaOMS subscribes to the delta logs of the configured databases/tables and pulls all the operational metrics written out during Delta table writes. These metrics are enriched with additional information ( like path, file name, commit timestamp etc.), processed to build snapshots over time and persisted into different tables as actions and commit information. Refer to the Delta Transaction Log Protocol for more details on Actions and CommitInfo.\nHigh-Level Process Flow: How is the DeltaOMS executed DeltaOMS provides a jar and sample notebooks to help you setup , configure and create the required databases, tables and Databricks Jobs.These notebooks and jobs run on your environment to create a centralized operational metrics database, capture metrics and make it available for analytics.\nHow much does it cost ? DeltaOMS does not have any direct cost associated with it other than the cost to run the jobs on your environment.The overall cost will be determined primarily by the number of Delta Lakehouse objects tracked and frequency of the OMS data refresh. We have found that the additional insights gained from DeltaOMS helps reduce the total cost of ownership through better management and optimization of your data pipelines while providing much improved observability of the Delta Lakehouse.\nMore questions Refer to the FAQ and Developer Guide\nGetting Started Refer to the Getting Started guide\nDeploying / Installing / Using the Project The DeltaOMS solution is available through Maven. You can get the jar using the following maven co-ordinates : \u0026quot;com.databricks.labs\u0026quot; % \u0026quot;delta-oms_2.12\u0026quot; % \u0026quot;0.1.0\u0026quot;\nPlease follow the Getting Started guide for instructions on using DeltaOMS on Databricks environment.\nReleasing the Project DeltaOMS is released as a jar and notebooks for setting up Databricks jobs. It also provides few sample notebooks for typical analysis. Refer to the Getting Started guide for more details.\nProject Support Please note that all projects in the databrickslabs github account are provided for your exploration only, and are not formally supported by Databricks with Service Level Agreements (SLAs). They are provided AS-IS and we do not make any guarantees of any kind. Please do not submit a support ticket relating to any issues arising from the use of these projects.\nAny issues discovered through the use of this project should be filed as GitHub Issues on the Repo. They will be reviewed as time permits, but there are no formal SLAs for support.\nContributing See our CONTRIBUTING for more details.\n"
},
{
	"uri": "https://databrickslabs.github.io/delta-oms/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://databrickslabs.github.io/delta-oms/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]